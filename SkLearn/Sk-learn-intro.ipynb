{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ML and SK learn Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "following this video: https://www.youtube.com/watch?v=M9Itm95JzL0 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazon review Data: http://jmcauley.ucsd.edu/data/amazon/\n",
    "download json books small data: click on book file, click raw then save page as all files which will save as json: https://github.com/KeithGalli/sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also follow this: https://jakevdp.github.io/PythonDataScienceHandbook/05.02-introducing-scikit-learn.html "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets make this a class that we can work with for object oriented programming later \n",
    "class Review:\n",
    "    def __init__(self, text, score):\n",
    "        self.text = text\n",
    "        self.score = score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I bought both boxed sets, books 1-5.  Really a great series!  Start book 1 three weeks ago and just finished book 5.  Sloane Monroe is a great character and being able to follow her through both private life and her PI life gets a reader very involved!  Although clues may be right in front of the reader, there are twists and turns that keep one guessing until the last page!  These are books you won't be disappointed with.\n",
      "\n",
      " 5.0\n"
     ]
    }
   ],
   "source": [
    "#our goal is to get the reviews of the text and the sentiment to analyze\n",
    "import json \n",
    "\n",
    "file_name = './books_small_10000.json' #path to where the file is, it is in the same folder\n",
    "with open(file_name) as f:\n",
    "    for line in f:\n",
    "        #print(line) #to look at raw text, see \"reviewText\": for review and use this\n",
    "        review = json.loads(line) #want to get the review from this raw text. use print(line) to see raw text\n",
    "        print(review[\"reviewText\"])\n",
    "        print(\"\\n\", review['overall'])\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('It was a decent read.. typical story line. Nothing unsavory as so many are. Just a slice of life, plausible.',\n",
       " 3.0)"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now lets compile the reviews into a list to work with \n",
    "reviews = []\n",
    "with open(file_name) as f:\n",
    "    for line in f:\n",
    "        #print(line) #to look at raw text, see \"reviewText\": for review and use this\n",
    "        review = json.loads(line) #want to get the review from this raw text. use print(line) to see raw text\n",
    "        #print(review[\"reviewText\"])\n",
    "        #print(\"\\n\", review['overall'])\n",
    "        reviews.append((review['reviewText'], review['overall']))\n",
    "reviews[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#with instantiation of class Review which is review, we should be able to call score parameter of reviews by attributing \n",
    "#Review[review['reviewText']], etc. \n",
    "reviews = []\n",
    "with open(file_name) as f:\n",
    "    for line in f:\n",
    "        #print(line) #to look at raw text, see \"reviewText\": for review and use this\n",
    "        review = json.loads(line) #want to get the review from this raw text. use print(line) to see raw text\n",
    "        #print(review[\"reviewText\"])\n",
    "        #print(\"\\n\", review['overall'])\n",
    "        reviews.append(Review(review['reviewText'], review['overall']))\n",
    "#reviews[4]\n",
    "\n",
    "reviews[4].score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'It was a decent read.. typical story line. Nothing unsavory as so many are. Just a slice of life, plausible.'"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews[4].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now let's add sentiment since this is what we want to work with later\n",
    "#lets make this a class that we can work with for object oriented programming later \n",
    "#first define class sentiment that we can use to consistently attribute the sentiment values\n",
    "import random \n",
    "\n",
    "class Sentiment:\n",
    "    Positive = \"Positive\"\n",
    "    Negative = \"Negative\"\n",
    "    Neutral = \"Neutral\"\n",
    "\n",
    "class Review:\n",
    "    def __init__(self, text, score):\n",
    "        self.text = text\n",
    "        self.score = score\n",
    "        self.sentiment = self.get_sentiment() #this will now return whatever the getter below does\n",
    "    \n",
    "    def get_sentiment(self):\n",
    "        if self.score >= 4:\n",
    "            return Sentiment.Positive\n",
    "        elif self.score <= 2:\n",
    "            return Sentiment.Negative\n",
    "        elif self.score == 3:\n",
    "            return Sentiment.Neutral\n",
    "#create a container to normalize and even out the negative and positive data so that it isn't so skewed\n",
    "class ReviewContainer:\n",
    "    def __init__(self, reviews):\n",
    "        self.reviews = reviews\n",
    "    \n",
    "    def get_text(self):\n",
    "        return [x.text for x in self.reviews] #x now becomes an object that was a compnenet of the object train_set\n",
    "    \n",
    "    def get_sentiment(self):\n",
    "        return [y.sentiment for y in self.reviews]\n",
    "    \n",
    "    def evenly_distribute(self):\n",
    "        negative = list(filter(lambda x: x.sentiment == Sentiment.Negative, self.reviews))\n",
    "        positive = list(filter(lambda x: x.sentiment == Sentiment.Positive, self.reviews))\n",
    "        positive_shrunk = positive[:len(negative)]\n",
    "        self.reviews = negative + positive_shrunk\n",
    "        random.shuffle(self.reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Neutral'"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#so now when you call instantiation of class Sentiment sentiment you get\n",
    "reviews[4].sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Prep Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will now turn text into quantitative vector so that we can better use it for ml since text sucks as input for ml models\n",
    "# let's use the bag of words method, let's split our reviews into a training set and a test set, search how to split sklearn\n",
    "len(reviews) #https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n",
    "from sklearn.model_selection import train_test_split \n",
    "#if you do shift+tab when inside the parenthesis of the method or function it will give you the same documentation\n",
    "#train_test_split() #try it out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "872"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set, test_set = train_test_split(reviews, test_size=0.33, random_state=42) \n",
    "train_container = ReviewContainer(train_set)\n",
    "test_container = ReviewContainer(test_set)\n",
    "train_container.evenly_distribute()\n",
    "test_container.evenly_distribute()\n",
    "len(cont.reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6700\n",
      "3300\n"
     ]
    }
   ],
   "source": [
    "#let's how this splitted to make sure our train_set variable is the training set which 2/3 of the 1000 reviews \n",
    "print(len(train_set))\n",
    "print(len(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.Review object at 0x000001A1F4D26278>\n"
     ]
    }
   ],
   "source": [
    "#now we used classes to work with this well, so let's get our text using our OOP classes of the objects train_set\n",
    "print(train_set[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"This was a real pleasure.  I really like that this story -- book 4 of the series -- doesn't follow a cookie-cutter formulaic plot like so many other serial novels do.  This volume was fresh, original and unpredictable.  The set up for book 5 was very nicely done and I'm eagerly looking forward to the next one.  I would give The Enemy Within six stars if I could.\""
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set[4].text\n",
    "#can use print(train_set[4].text) to return as non-string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.0"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set[4].score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Positive'"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set[4].sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive\n"
     ]
    }
   ],
   "source": [
    "print(train_set[4].sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interesting concept though I can't wrap my mind around the why for the con s piracy theory concerning the first born.  The plot was good and fast paced and the characters were well written.\n",
      "Positive\n",
      "436\n",
      "436\n"
     ]
    }
   ],
   "source": [
    "#we want to pass sentiment and text into our bag of words vectorizer, it will have an x independent variable and y dependent variable\n",
    "#so we want to split the components the words or text as train_set_x and sentiment as train_set_y where the sentiment is the output \n",
    "# of the model based on the text coming in \n",
    "train_container.evenly_distribute() #should make the positive and negative values even\n",
    "train_set_x = train_container.get_text() #x now becomes an object that was a compnenet of the object train_set\n",
    "train_set_y = train_container.get_sentiment()\n",
    "test_set_x = test_container.get_text()\n",
    "test_set_y = test_container.get_sentiment()\n",
    "\n",
    "print(train_set_x[4])\n",
    "print(train_set_y[4])\n",
    "#so now we see that the text in train_set is the text and same for sentiment\n",
    "print(train_set_y.count(Sentiment.Positive))\n",
    "print(train_set_y.count(Sentiment.Negative))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Bag of words vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#https://scikit-learn.org/stable/modules/feature_extraction.html \n",
    "Raw data, a sequence of symbols cannot be fed directly to the algorithms themselves as most of them expect numerical feature\n",
    "vectors with a fixed size rather than the raw text documents with variable length.\n",
    "In order to address this, scikit-learn provides utilities for the most common ways to extract numerical features from text content, namely:\n",
    "\n",
    "    tokenizing strings and giving an integer id for each possible token, for instance by using white-spaces and punctuation as token separators.\n",
    "\n",
    "    counting the occurrences of tokens in each document.\n",
    "\n",
    "    normalizing and weighting with diminishing importance tokens that occur in the majority of samples / documents.\n",
    "\n",
    "In this scheme, features and samples are defined as follows:\n",
    "\n",
    "    each individual token occurrence frequency (normalized or not) is treated as a feature.\n",
    "\n",
    "    the vector of all the token frequencies for a given document is considered a multivariate sample.\n",
    "\n",
    "A corpus of documents can thus be represented by a matrix with one row per document and one column per token (e.g. word) occurring in the corpus.\n",
    "\n",
    "We call vectorization the general process of turning a collection of text documents into numerical feature vectors. This specific strategy (tokenization, counting and normalization) is called the Bag of Words or “Bag of n-grams” representation. Documents are described by word occurrences while completely ignoring the relative position information of the words in the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<872x8906 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 53647 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "train_x_vectors = vectorizer.fit_transform(train_set_x) #this does a fit of the data and transforms it at the once to save the step\n",
    "test_x_vectors = vectorizer.transform(test_set_x) #don't want to fit this bc it is the test data \n",
    "train_x_vectors #this creates a matrix or array of all the text from the text portion of the review list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x8906 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 27 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x_vectors[4] #see now that there is a matrix storing the text in vectorized form which is one vector with 7372 components or rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 7929)\t5\n",
      "  (0, 5939)\t1\n",
      "  (0, 423)\t2\n",
      "  (0, 3177)\t1\n",
      "  (0, 3432)\t1\n",
      "  (0, 8608)\t1\n",
      "  (0, 1362)\t1\n",
      "  (0, 4190)\t1\n",
      "  (0, 1629)\t1\n",
      "  (0, 7986)\t1\n",
      "  (0, 1219)\t1\n",
      "  (0, 8827)\t1\n",
      "  (0, 5260)\t1\n",
      "  (0, 5092)\t1\n",
      "  (0, 544)\t1\n",
      "  (0, 8715)\t1\n",
      "  (0, 1628)\t1\n",
      "  (0, 5892)\t1\n",
      "  (0, 7949)\t1\n",
      "  (0, 1631)\t1\n",
      "  (0, 3087)\t1\n",
      "  (0, 1009)\t1\n",
      "  (0, 2972)\t1\n",
      "  (0, 5646)\t1\n",
      "  (0, 8670)\t1\n",
      "  (0, 8665)\t1\n",
      "  (0, 8843)\t1\n"
     ]
    }
   ],
   "source": [
    "#lets see what this vector looks like with the tolkiens to store the count of frequency of words, etc. \n",
    "print(train_x_vectors[4]) #it is counting the frequency on the right of occurence of each position that is non-zero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to learn some of the theory to get better at how to choose your classication or classify your data, see here this link for \n",
    "comparisons of classfications to choose: https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Positive'], dtype='<U8')"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lets SVM first - linear SVM \n",
    "from sklearn import svm\n",
    "\n",
    "clf_svm = svm.SVC(kernel='linear')\n",
    "\n",
    "clf_svm.fit(train_x_vectors, train_set_y)\n",
    "test_set_x[4]\n",
    "clf_svm.predict(test_x_vectors[4])#looks positive like the review in the test_set_x[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Positive'], dtype='<U8')"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "clf_dec = DecisionTreeClassifier()\n",
    "clf_dec.fit(train_x_vectors, train_set_y) #taking text tolkien vectors and fitting with sentiment values\n",
    "test_set_x[4]\n",
    "clf_dec.predict(test_x_vectors[4]) #see if it is the same, should be positive review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Positive'], dtype='<U8')"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#let's try gaussian naive bayes\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "clf_gnb = GaussianNB()\n",
    "\n",
    "clf_gnb.fit(train_x_vectors.toarray(), train_set_y)\n",
    "clf_gnb.predict(test_x_vectors[4].toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Positive'], dtype='<U8')"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf_log = LogisticRegression()\n",
    "\n",
    "clf_log.fit(train_x_vectors.toarray(), train_set_y)\n",
    "clf_log.predict(test_x_vectors[4].toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Evaluation\n",
    "<h3>Lets now test the predictions more comprehensively "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7980769230769231\n",
      "0.6346153846153846\n",
      "0.6346153846153846\n",
      "0.8149038461538461\n"
     ]
    }
   ],
   "source": [
    "#use score to test the x vs y score: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html\n",
    "#below are the mean accuracies \n",
    "print(clf_svm.score(test_x_vectors, test_set_y))#82% not bad!\n",
    "print(clf_dec.score(test_x_vectors, test_set_y))\n",
    "print(clf_gnb.score(test_x_vectors.toarray(), test_set_y))\n",
    "print(clf_log.score(test_x_vectors, test_set_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best one looks like the logistic regression!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\owner\\anaconda3\\envs\\ml_env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1465: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  average, \"true nor predicted\", 'F-score is', len(true_sum)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8028169  0.         0.79310345]\n",
      "[0.63809524 0.         0.63106796]\n",
      "[0.59574468 0.         0.66666667]\n",
      "[0.82051282 0.         0.808933  ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\owner\\anaconda3\\envs\\ml_env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1465: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  average, \"true nor predicted\", 'F-score is', len(true_sum)\n"
     ]
    }
   ],
   "source": [
    "# F1 Scores\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "print(f1_score(test_set_y, clf_svm.predict(test_x_vectors), average=None, labels=[Sentiment.Positive, Sentiment.Neutral, Sentiment.Negative]))\n",
    "print(f1_score(test_set_y, clf_dec.predict(test_x_vectors), average=None, labels=[Sentiment.Positive, Sentiment.Neutral, Sentiment.Negative]))\n",
    "print(f1_score(test_set_y, clf_gnb.predict(test_x_vectors.toarray()), average=None, labels=[Sentiment.Positive, Sentiment.Neutral, Sentiment.Negative]))\n",
    "print(f1_score(test_set_y, clf_log.predict(test_x_vectors), average=None, labels=[Sentiment.Positive, Sentiment.Neutral, Sentiment.Negative]))\n",
    "\n",
    "#can see most do good on positive but terrible on neutral and negative, second round we see big improvement in the negative sentiment\n",
    "#once we use the evenly distribute container to have a more even amount of postive and negative values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    Positive\n",
      "1    Positive\n",
      "2    Negative\n",
      "3    Negative\n",
      "4    Positive\n",
      "dtype: object\n",
      "count          872\n",
      "unique           2\n",
      "top       Negative\n",
      "freq           436\n",
      "dtype: object\n",
      "<bound method IndexOpsMixin.value_counts of 0      Positive\n",
      "1      Positive\n",
      "2      Negative\n",
      "3      Negative\n",
      "4      Positive\n",
      "         ...   \n",
      "867    Negative\n",
      "868    Negative\n",
      "869    Negative\n",
      "870    Positive\n",
      "871    Positive\n",
      "Length: 872, dtype: object>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "872"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#there might be bias in our data or something to explain this, lets look at the data a little further to see\n",
    "import pandas as pd\n",
    "df = pd.Series(train_set_y)\n",
    "print(df.head())\n",
    "print(df.describe())\n",
    "print(df.value_counts)\n",
    "len(df) #we can see here already that most of the values are rated positive, defintely a bias here in the training \n",
    "#second round we now have about 50/50 distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Test model \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Positive', 'Positive', 'Negative'], dtype='<U8')"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set = [\"this book is great\", \"fantastic story line\", \"decent plot but sad ending\"]\n",
    "vect_test_set = vectorizer.transform(test_set)\n",
    "clf_svm.predict(vect_test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Looks pretty accurate to me!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Negative' 'Positive' 'Negative']\n",
      "['Negative' 'Negative' 'Negative']\n",
      "['Negative' 'Negative' 'Negative']\n",
      "['Negative' 'Positive' 'Negative']\n"
     ]
    }
   ],
   "source": [
    "#lets test negative sentences and use the other ML algos\n",
    "test = ['not reading again', \"this wasn't the best book\", \"not awesome to read\"]\n",
    "vect_test = vectorizer.transform(test)\n",
    "print(clf_svm.predict(vect_test)) #got it wrong which is surprising\n",
    "print(clf_gnb.predict(vect_test.toarray())) #got it right even our accuracy is not the best with this model\n",
    "print(clf_log.predict(vect_test))#got them all right and did have the highest accuracy\n",
    "print(clf_dec.predict(vect_test))#got wrong as expected since it had the worst accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Based on these results I would say that the logistic regression is the best model to use! But we see an accuracy problem arise with these negative statements so let's drive up those mean accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Let's use Tfid and grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=SVC(),\n",
       "             param_grid={'C': (1, 4, 8, 16, 32), 'kernel': ('linear', 'rbf')})"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#instead of using the count vectorizer let's use the tfidvectorizer, we are parameter tuning with gridsearch\n",
    "#new_vectorizer = TfidVectorizer()\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "#gridsearch helps us find the best values to use in the SVC parameters, let's focus on C and kernel, rememebr shift+tab to see\n",
    "#create parameters to test with gridsearch \n",
    "parameters = {'kernel': ('linear', 'rbf'), 'C': (1,4,8,16,32)}\n",
    "svc = svm.SVC()\n",
    "clf_svm_2 = GridSearchCV(svc, parameters, cv=5)\n",
    "clf_svm_2.fit(train_x_vectors, train_set_y)\n",
    "#looks like it changed it to rbf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7764423076923077\n"
     ]
    }
   ],
   "source": [
    "print(clf_svm_2.score(test_x_vectors, test_set_y)) #looks like it did worse, more work needs to be done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Overall, pretty accurate models! ML is cool 😎"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Saving model \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "#choose the most accurate logistic regression model \n",
    "with open('./models/sentiment_classifier.pkl', 'wb') as f: #wb is the write buffer\n",
    "    pickle.dump(clf_log, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to load the model do this:\n",
    "with open('./models/sentiment_classifier.pkl', 'rb') as f:\n",
    "    loaded_clf = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Positive']\n",
      "['Negative' 'Negative' 'Negative']\n",
      "0.8149038461538461\n"
     ]
    }
   ],
   "source": [
    "print(loaded_clf.predict(test_x_vectors[0]))\n",
    "print(loaded_clf.predict(vect_test))\n",
    "print(loaded_clf.score(test_x_vectors, test_set_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> As we can see this is the same classifier as the logistic regression that we just loaded "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "ml_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
